# 决策树
基于**特征**对实例进行分类的过程

利用训练数据，根据损失函数最小化原则建立决策树模型

三个步骤：特征选择、决策树生成、决策树修剪

## 决策树模型
结点有内部节点和叶节点
- 内部节点表示一个特征或属性
- 叶节点表示一个类

决策树分类步骤
- 对实例的某一特征进行测试，将实例分配到其叶子节点，将实例分到叶子节点的类中

### 决策树学习
本质是从训练数据集中归纳出一组分类规则
- 需要的是一个与训练数据集矛盾较小的决策树
- 决策树学习是由训练数据估计条件概率模型

损失函数：正则化的极大似然函数

### 详细步骤
决策树生成
- 构建根节点，所有训练数据存放在根节点
- 选择一个最优特性，划分子集
- 正确分类则构建叶子节点，将子集分到对应的叶子节点，如果有子集没被正确分类，则回到上一步
- 对子集选择最优特性，递归下去
- 只需考虑局部最优

过拟合处理：决策树剪枝
- 自下而上剪枝，使决策树变得简单
- 需要考虑全局最优

## 特征选择
决定使用哪个特征来划分特征空间

### 准则：信息增益
- 熵 表示随机变量的不确定性度量
- `H(X) = -sum(pi·logpi)`
- 0.5 时最大，不确定性最大，0或1最小

信息增益表示得知特征X后（也就是条件概率），能使得类Y的不确定性减小的程度

g(D,A) = H(D) - H(D|A)
- D 训练数据集
- A 特征
- g 互信息 等价于信息增益

基于信息增益的特征选择方法：对训练集D，计算每个特征的信息增益，比较大小，选择最大的

### 准则：信息增益比
H(D)大的信息增益会大，解决方法
- 信息增益比gR(D,A) = g(D,A)/H(D)

## 决策树生成

###ID3 算法
根节点起对所有可能的特征算信息增益，选择信息增益最大为结点特征，该特征的不同值建子节点。对子节点递归调用。

容易过拟合  

### C4.5
- 信息增益改成信息增益比

## 决策树剪枝
过拟合了，决策树过于复杂。

算法：极小化决策树整体损失函数

`C(T) = 所有节点sum(节点中的样本点 × 经验熵) + α|T|`
- C(T) 预测误差
- |T| 模型复杂度
- α 控制两者之间的影响，越大模型越简单、

步骤：
- 计算每个节点的经验熵
- 递归从叶节点往上回缩
- 比较回缩前后的损失函数值，直至不能继续，得到损失函数最小的子树

## CART 算法
分类与回归树（classification and regression tree)，应用广泛的决策树学习方法
- 假设决策树是二叉树（节点取值为是和否）
- 基尼指数来计算不确定度
  - 表示在样本集合中一个随机选中的样本被分错的概率
    - = 样本被选中的概率(pk) * 样本被分错的概率(1-pk)
  - G 越大，数据的不确定性越高；
  - G 越小，数据的不确定性越低；
  - G = 0，数据集中的所有样本都是同一类别；



 